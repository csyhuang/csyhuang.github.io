---
layout: post
title: Papers on architecture of Recurrent Neural Networks (RNN)
comments: true
tags: ['nlp']
---

Bookmarking some papers mentioned in Andrew Ng's course [Sequence Models](https://www.coursera.org/learn/nlp-sequence-models):

### Gate Recurrent Unit (GRU)

- Cho, K., Van MerriÃ«nboer, B., Bahdanau, D., & Bengio, Y. (2014). [On the properties of neural machine translation: Encoder-decoder approaches](https://arxiv.org/abs/1409.1259). *arXiv preprint arXiv:1409.1259*.

- Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). [Empirical evaluation of gated recurrent neural networks on sequence modeling](https://arxiv.org/abs/1412.3555). *arXiv preprint arXiv:1412.3555*.

### Long short-term memory (LSTM)

- Hochreiter, S., & Schmidhuber, J. (1997). [Long short-term memory](https://www.bioinf.jku.at/publications/older/2604.pdf). *Neural computation*, 9(8), 1735-1780.

(More to update)