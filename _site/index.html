<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Blog &middot; Clare S. Y. Huang
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>PhD in Geophysical Sciences (UChicago). Love coding, music and writing.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>
    <a class="sidebar-nav-item" href="/about_me.html" >About me</a>    

    

    
    
      
        
      
    
      
        
      
    
      
        
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="/bookshelf/">Bookshelf</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/climate_tools/">Climate Tools</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/ds_notes/">Data Science Notes</a>
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/publications/">Publications</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/pyspark_solutions/">PySpark/SQL Solutions</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/research/">Academic Research</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/tools/">Handy Tools</a>
        
      
    

<!--    <a class="sidebar-nav-item" href="/archive/v1.0.0.zip">Download</a> -->

    <a class="sidebar-nav-item" href="http://github.com/csyhuang/"  target="_blank">Github</a>
<!--    <span class="sidebar-nav-item">Currently v1.0.0</span> --->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2020 Shao Ying (Clare) Huang. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Clare S. Y. Huang</a>
            <small>Data Scientist | Atmospheric Dynamicist</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<div class="row">
  <div class="col-sm-8">
<!-- 
  <p class="influidcontainer">I have just completed my Ph.D degree in Geophysical Sciences in the University of Chicago and currently work as a postdoctoral research assistant in Professor Noboru Nakamura's group.
  My <a href="/research">dissertation</a> focuses on developing an atmospheric Rossby wave diagnostic theory that quantifies the severity of extreme weather events. I am now working on the search of predictive features of atmospheric blocking using the local wave activity theory I developed and some machine learning techniques.
  Check out my <a href="https://github.com/csyhuang/hn2016_falwa" target="blank">python library</a> to implement this diagnosis on climate data!
  </p>
  <p class="influidcontainer">
  I also participated in the <a href="http://insightdatascience.com">Insight Data Science Fellowship Program</a> in 2017 summer as a Health Data Fellow to learn about data science and health industry.
  Currently, I work in the Research Analytics team at <a href="http://tempus.com/">Tempus</a>.
  </p>
 -->
  <p class="influidcontainer">
  I love to share what I've learnt with others. Check out my blog posts and notes about my 
  academic research, as well as technical solutions on software engineering and data 
  science challenges.
  </p>  
  </div>
  

  <div class="col-sm-4">
<!-- 

    <img src="/assets/img/profile_pic_square.jpg">
    
 -->
    <div align="right">
      <p><a href="http://csyhuang.github.io/feed.xml"  target="_blank"><i class="fa fa-rss" aria-hidden="true"></i> RSS Feed </a></p>
      <p><a href="mailto:csyhuang@uchicago.edu"><i class="fa fa-envelope" aria-hidden="true"></i> Email </a></p>
    </div>
    
    
  </div>
</div>


<!-- 
Welcome to my personal homepage! Please use the <b>left-side menu</b> to navigate to pages related to my academic work. This index page is maintained in a blog-like manner that I'll constantly put in things I've learnt, and updated content of this website.
  </div>
</div>
 -->
<div class="clear"></div>
<hr>

<div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2020/09/17/split-vector-to-columns/">
        Split a vector/list in a pyspark DataFrame into columns
      </a>
    </h1>

    <span class="post-date">17 Sep 2020</span>

    
        <h2 id="split-an-array-column">Split an array column</h2>
<p>To split a column with arrays of strings, e.g. a DataFrame that looks like,</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+---------+
|   strCol|
+---------+
|[A, B, C]|
+---------+
</code></pre></div></div>
<p>into separate columns, the following code without the use of UDF works.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">select</span><span class="p">([</span><span class="n">F</span><span class="p">.</span><span class="n">col</span><span class="p">(</span><span class="s">"strCol"</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)])</span>
<span class="n">df2</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p>Output:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+---------+---------+---------+
|strCol[0]|strCol[1]|strCol[2]|
+---------+---------+---------+
|        A|        B|        C|
+---------+---------+---------+
</code></pre></div></div>

<h2 id="split-a-vector-column">Split a vector column</h2>
<p>To split a column with doubles stored in DenseVector format, e.g. a DataFrame that looks like,</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+-------------+
|       intCol|
+-------------+
|[2.0,3.0,4.0]|
+-------------+
</code></pre></div></div>
<p>one have to construct a UDF that does the convertion of DenseVector to array(python list) first:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">ArrayType</span><span class="p">,</span> <span class="n">DoubleType</span>

<span class="k">def</span> <span class="nf">split_array_to_list</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">to_list</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">v</span><span class="p">.</span><span class="n">toArray</span><span class="p">().</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">udf</span><span class="p">(</span><span class="n">to_list</span><span class="p">,</span> <span class="n">ArrayType</span><span class="p">(</span><span class="n">DoubleType</span><span class="p">()))(</span><span class="n">col</span><span class="p">)</span>

<span class="n">df3</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="n">split_array_to_list</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">col</span><span class="p">(</span><span class="s">"intCol"</span><span class="p">)).</span><span class="n">alias</span><span class="p">(</span><span class="s">"split_int"</span><span class="p">))</span>\
    <span class="p">.</span><span class="n">select</span><span class="p">([</span><span class="n">F</span><span class="p">.</span><span class="n">col</span><span class="p">(</span><span class="s">"split_int"</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)])</span>
<span class="n">df3</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p>Output:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+------------+------------+------------+
|split_int[0]|split_int[1]|split_int[2]|
+------------+------------+------------+
|         2.0|         3.0|         4.0|
+------------+------------+------------+
</code></pre></div></div>

    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2020/09/14/dense-rank-and-character-order/">
        Ranking hierarchical labels with SQL
      </a>
    </h1>

    <span class="post-date">14 Sep 2020</span>

    
        <p>To give indices to hierarchical labels, I can use <code class="highlighter-rouge">DENSE_RANK()</code> or <code class="highlighter-rouge">RANK()</code> depending on the situation. 
For example, if I have a DataFrame that looks like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+------+----------+
|Fridge|    Fruits|
+------+----------+
|     A|     apple|
|     B|    orange|
|     C|     apple|
|     D|     pears|
|     C|Watermelon|
+------+----------+
</code></pre></div></div>

<p>The following SQL code</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span>
    <span class="n">Fridge</span><span class="p">,</span>
    <span class="n">Fruits</span><span class="p">,</span>
    <span class="n">DENSE_RANK</span><span class="p">()</span> <span class="n">OVER</span> <span class="p">(</span><span class="k">ORDER</span> <span class="k">BY</span> <span class="n">Fridge</span><span class="p">,</span> <span class="n">Fruits</span><span class="p">)</span> <span class="k">AS</span> <span class="n">Loc_id</span><span class="p">,</span>
    <span class="n">DENSE_RANK</span><span class="p">()</span> <span class="n">OVER</span> <span class="p">(</span><span class="k">ORDER</span> <span class="k">BY</span> <span class="n">Fridge</span><span class="p">)</span> <span class="k">AS</span> <span class="n">Fridge_id_dense</span><span class="p">,</span>
    <span class="n">RANK</span><span class="p">()</span> <span class="n">OVER</span> <span class="p">(</span><span class="k">ORDER</span> <span class="k">BY</span> <span class="n">Fridge</span><span class="p">)</span> <span class="k">AS</span> <span class="n">Fridge_id</span><span class="p">,</span>
    <span class="n">DENSE_RANK</span><span class="p">()</span> <span class="n">OVER</span> <span class="p">(</span><span class="k">ORDER</span> <span class="k">BY</span> <span class="n">Fruits</span><span class="p">)</span> <span class="k">AS</span> <span class="n">Fruit_id_dense</span><span class="p">,</span>
    <span class="n">RANK</span><span class="p">()</span> <span class="n">OVER</span> <span class="p">(</span><span class="k">ORDER</span> <span class="k">BY</span> <span class="n">Fruits</span><span class="p">)</span> <span class="k">AS</span> <span class="n">Fruit_id</span>
<span class="k">FROM</span> <span class="n">fridge_list</span>
</code></pre></div></div>

<p>would yield the following table</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+------+----------+------+---------------+---------+--------------+--------+
|Fridge|    Fruits|Loc_id|Fridge_id_dense|Fridge_id|Fruit_id_dense|Fruit_id|
+------+----------+------+---------------+---------+--------------+--------+
|     A|     apple|     1|              1|        1|             2|       2|
|     B|    orange|     2|              2|        2|             3|       4|
|     C|Watermelon|     3|              3|        3|             1|       1|
|     C|     apple|     4|              3|        3|             2|       2|
|     D|     pears|     5|              4|        5|             4|       5|
+------+----------+------+---------------+---------+--------------+--------+

</code></pre></div></div>

    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2020/07/14/hn2016_falwa-release0.4.1/">
        Minor release of my python package + release procedures
      </a>
    </h1>

    <span class="post-date">14 Jul 2020</span>

    
        <p>[<a href="https://github.com/csyhuang/hn2016_falwa/releases/tag/0.4.1">hn2016_falwa Release 0.4.1</a>] A minor release of my python package <a href="https://github.com/csyhuang/hn2016_falwa">hn2016_falwa</a> is published. Thanks <a href="https://github.com/chpolste">Christopher Polster</a> for submitting a pull request that fixes the interface of <code class="highlighter-rouge">BarotropicField</code>. Moreover, I added procedures to process masked array in <code class="highlighter-rouge">QGField</code> such that it can be conveniently used to process ERA5 data which is stored as masked array in netCDF files.</p>

<p>As a memo to myself - procedures for a release (which I often forget and have to google üòÖ):</p>
<ul>
  <li>Update version number in <code class="highlighter-rouge">setup.py</code>, <code class="highlighter-rouge">readme.md</code> and documentation pages.</li>
  <li>Add a (light-weighted) tag to the commit: <code class="highlighter-rouge">git tag &lt;tagname&gt;</code>.</li>
  <li>Not only push the commits but also the tag by <code class="highlighter-rouge">git push origin &lt;tagname&gt;</code>.</li>
</ul>

<p>If I have time, I would update the version on PYPI too:</p>
<ul>
  <li>Create the <code class="highlighter-rouge">dist/</code> directory and the installation files: <code class="highlighter-rouge">python3 setup.py sdist bdist_wheel</code></li>
  <li>Upload the package: <code class="highlighter-rouge">python3 -m twine upload --repository testpypi dist/*</code></li>
</ul>

    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2020/06/30/bulk-download-of-era5/">
        Bulk download of ERA5 data from CDSAPI
      </a>
    </h1>

    <span class="post-date">30 Jun 2020</span>

    
        <p>I wrote a sample script to download ERA5 reanalysis data via CDSAPI month by month. <a href="https://github.com/csyhuang/download_era5">Here is the GitHub repo</a> with instructions how to use it.</p>

    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2020/04/18/spark-definitive-guide-reading-note/">
        Reading Notes on Spark - The Definitive Guide
      </a>
    </h1>

    <span class="post-date">18 Apr 2020</span>

    
        <div>
            <p>I am reading the book <a href="https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/">Spark: The Definitive Guide</a> by Bill Chambers, Matei Zaharia. Here are my reading notes:

        </div>
        <input type="checkbox" class="read-more-state" id="/2020/04/18/spark-definitive-guide-reading-note/"/>
        <div class="read-more">
            </p>

<ul>
  <li>
    <p><a href="/spark_the_def_guide/ch1">Ch.1 - What is Apache Spark?</a></p>
  </li>
  <li>
    <p><a href="/spark_the_def_guide/ch2">Ch.2 - A Gentle Introduction to Spark</a></p>
  </li>
</ul>


        </div>
        <label for="/2020/04/18/spark-definitive-guide-reading-note/" class="read-more-trigger"></label>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2020/04/17/add-read-more-on-front-page/">
        READ MORE button via jekyll
      </a>
    </h1>

    <span class="post-date">17 Apr 2020</span>

    
        <div>
            <p>Found a workable solution adding ‚Äúread more‚Äù button to jekyll posts from <a href="https://jonnylangefeld.com/blog/how-to-add-a-read-more-button-that-doesnt-suck-to-your-jekyll-blog">Jonny Langefeld‚Äôs blog post</a>. üòÑ Thanks for the solution!

        </div>
        <input type="checkbox" class="read-more-state" id="/2020/04/17/add-read-more-on-front-page/"/>
        <div class="read-more">
            
Here we go. üòâ</p>

        </div>
        <label for="/2020/04/17/add-read-more-on-front-page/" class="read-more-trigger"></label>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2020/04/16/faster-outer-join-by-two-left-joins/">
        More efficient way to do outer join with large dataframes
      </a>
    </h1>

    <span class="post-date">16 Apr 2020</span>

    
        <p>Today I learned from a colleague the way of doing <code class="highlighter-rouge">outer join</code> of large dataframes more efficiently: instead of doing the <code class="highlighter-rouge">outer join</code>, you can first <code class="highlighter-rouge">union</code> the key column, and then implement <code class="highlighter-rouge">left join</code> twice. I have done an experiment myself on the cluster with two dataframes (<code class="highlighter-rouge">df1</code> and <code class="highlighter-rouge">df2</code>) - each dataframe has ~10k rows, and there is only ~10% of overlap(i.e. an inner-join would result in ~1k rows).</p>

<p>The usual way of doing outer join would be like:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df3</span> <span class="o">=</span> <span class="n">df1</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s">'outer'</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">'id'</span><span class="p">).</span><span class="n">drop_duplicates</span><span class="p">()</span>
</code></pre></div></div>

<p>Here is an equivalent way(I call it <em>union-left</em> here) that takes less time to compute:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df3</span> <span class="o">=</span> <span class="n">df1</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">'id'</span><span class="p">).</span><span class="n">union</span><span class="p">(</span><span class="n">df2</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">'id'</span><span class="p">))</span>
<span class="n">df3</span> <span class="o">=</span> <span class="n">df3</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">x1_df</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s">'left'</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">'id'</span><span class="p">)</span>
<span class="n">df3</span> <span class="o">=</span> <span class="n">df3</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">x2_df</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s">'left'</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">'id'</span><span class="p">).</span><span class="n">drop_duplicates</span><span class="p">()</span>
</code></pre></div></div>

<p>The distribution of IDs in the two dataframes:</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>df1 only</th>
      <th>overlap</th>
      <th>df2 only</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td># of ids</td>
      <td>8625</td>
      <td>914</td>
      <td>8623</td>
    </tr>
  </tbody>
</table>

<p>Here is the distribution of computing times for <code class="highlighter-rouge">inner</code>, <code class="highlighter-rouge">left</code>, <code class="highlighter-rouge">outer</code>, <code class="highlighter-rouge">right</code> and <code class="highlighter-rouge">union-left</code>(that gives same results as <code class="highlighter-rouge">outer</code>) joins(I repeated each join 20 times):</p>

<p><img style="float: center;" src="/assets/img/types_of_join_computing_time.png" /></p>

<p>For these sizes of dataframes, the <code class="highlighter-rouge">union-left</code> join is on average ~20% faster than the equivalent <code class="highlighter-rouge">outer</code> join.</p>

    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2020/04/09/hn2016_falwa-release0.4.0/">
        Local wave activity calculation for Southern Hemisphere available in release0.4.0
      </a>
    </h1>

    <span class="post-date">09 Apr 2020</span>

    
        <p>[hn2016_falwa Release 0.4.0] üòÑI am happy to announce that the climate data analysis in Nakamura and Huang(2018, Science) for the southern hemisphere is also available <a href="https://github.com/csyhuang/hn2016_falwa">on GitHub</a> now! (Finally, I have time to do a thorough check of the code and make the release‚Ä¶)</p>

<p>Check out the release note for enhanced functionality:<br />
<a href="https://github.com/csyhuang/hn2016_falwa/releases/tag/0.4.0">https://github.com/csyhuang/hn2016_falwa/releases/tag/0.4.0</a></p>

<p>The documentation page has been fixed too:<br />
<a href="https://csyhuang.github.io/hn2016_falwa">https://csyhuang.github.io/hn2016_falwa</a></p>

<p>Jupyter notebook demonstrating the usage of the functions:<br />
<a href="https://github.com/csyhuang/hn2016_falwa/blob/master/examples/nh2018_science/demo_script_for_nh2018.ipynb">https://github.com/csyhuang/hn2016_falwa/blob/master/examples/nh2018_science/demo_script_for_nh2018.ipynb</a></p>

<p>If you have any questions/issues regarding the usage of the package, feel free to post on the <a href="https://github.com/csyhuang/hn2016_falwa/issues">Issue page</a> of the repo! I will help you fix them soon as I can!</p>

    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2020/02/07/tips-for-writing-more-efficient-sql/">
        Tips for writing more efficient SQL
      </a>
    </h1>

    <span class="post-date">07 Feb 2020</span>

    
        <p>Learned from colleagues some points to pay attention to when writing SQL queries. (This post will be updated from time to time.)</p>

<h1 id="partitioned-tables">Partitioned tables</h1>

<p><strong>Always</strong> specify the partition in the <code class="highlighter-rouge">where</code> clause. If you have to retrieve data from several partitions, loop through it one-by-one.</p>

<h1 id="distinct-elements">Distinct elements</h1>

<p>Note that the two queries</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="k">DISTINCT</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="k">FROM</span> <span class="n">table_X</span>
</code></pre></div></div>

<p>and</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="n">item</span> <span class="k">FROM</span> <span class="n">table_X</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">item</span>
</code></pre></div></div>

<p>give the same result. However, the second SQL query will be executed faster. There is no difference calling <code class="highlighter-rouge">distinct</code> and <code class="highlighter-rouge">group by</code> via (py)spark though.</p>

<h1 id="join-vs-where">JOIN v.s. WHERE</h1>

<p>Always use <code class="highlighter-rouge">where</code> to filter the table to be joined to the smallest, e.g.</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="k">c</span><span class="p">.</span><span class="o">*</span> <span class="k">FROM</span> <span class="n">credit</span> <span class="k">c</span>
<span class="k">INNER</span> <span class="k">JOIN</span> <span class="p">(</span>
	<span class="k">SELECT</span> <span class="nb">date</span><span class="p">,</span> <span class="n">item</span> <span class="k">FROM</span> <span class="n">purchase</span>
	<span class="k">WHERE</span> <span class="nb">date</span> <span class="o">&gt;</span> <span class="mi">20190207</span>
<span class="p">)</span> <span class="n">p</span>
<span class="k">ON</span> <span class="k">c</span><span class="p">.</span><span class="nb">date</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nb">date</span>
<span class="k">WHERE</span> <span class="k">c</span><span class="p">.</span><span class="n">eligibility</span> <span class="o">=</span> <span class="k">True</span>
</code></pre></div></div>

<p>Note that the line <code class="highlighter-rouge">WHERE c.eligibility = True</code> is executed to filter the table <code class="highlighter-rouge">credit</code> before the <code class="highlighter-rouge">JOIN</code>. This shrinks the table <code class="highlighter-rouge">credit</code> to the smallest before joining.</p>

    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2019/12/20/comic-series-on-bouldering/">
        I started a comic series about bouldering (for fun)
      </a>
    </h1>

    <span class="post-date">20 Dec 2019</span>

    
        <p>This is not about work but <strong>for fun</strong> :D</p>

<p>I recently started learning bouldering (2.5 months till now?) and that‚Äôs lots of fun! I keep updating a comic series - the <a href="https://www.instagram.com/explore/tags/matchman_bouldering_series/">Matchman Bouldering Series</a> - on <a href="https://www.instagram.com/clarescookbook/">Instagram</a>/<a href="https://matchman-bouldering-series.tumblr.com/">Tumblr</a> that records the ideas I learned from friends in every session collaborating with my college friend <a href="https://www.instagram.com/martin_ka_hei/">Martin</a>.</p>

<p>Some previews‚Ä¶</p>

<p><img src="/assets/img/thumbnails-of-matchman.png" alt="Matchman Bouldering Series" /></p>

<p>If you are fond of bouldering and have practical/physics ideas to share, ping me and see if we can collaborate! :)</p>

    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2019/10/30/pandas_to_pyspark_before_0.19.2/">
        Conversion of pandas dataframe to pyspark dataframe with an older version of pandas
      </a>
    </h1>

    <span class="post-date">30 Oct 2019</span>

    
        <div>
            <p>Pandas dataframe can be converted to pyspark dataframe easily in the newest version of <code class="highlighter-rouge">pandas</code> after <code class="highlighter-rouge">v0.19.2</code>. If you are using an older version of pandas, you have to do a bit more work for such conversion as follows.</p>

<p>First, load the packages and initiate a spark session.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">,</span> <span class="n">Row</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span> \
        <span class="p">.</span><span class="n">master</span><span class="p">(</span><span class="s">"local"</span><span class="p">)</span> \
        <span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">"Pandas to pyspark DF"</span><span class="p">)</span> \
        <span class="p">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</code></pre></div></div>

        </div>
        <input type="checkbox" class="read-more-state" id="/2019/10/30/pandas_to_pyspark_before_0.19.2/"/>
        <div class="read-more">
            
<p>Here is an example of pandas dataframe to be converted.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'index'</span><span class="p">:[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">)],</span>
                   <span class="s">'alphabet'</span><span class="p">:[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="s">'pyspark'</span><span class="p">]})</span>
<span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>index</th>
      <th>alphabet</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>p</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>y</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>s</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>p</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>a</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5</td>
      <td>r</td>
    </tr>
    <tr>
      <th>6</th>
      <td>6</td>
      <td>k</td>
    </tr>
  </tbody>
</table>
</div>

<p>To convert it to a <code class="highlighter-rouge">pyspark</code> dataframe, one has to create a list of <code class="highlighter-rouge">Row</code> objects and pass it into <code class="highlighter-rouge">createDataFrame</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_pyspark</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([</span>
    <span class="n">Row</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s">'index'</span><span class="p">],</span> <span class="n">alphabet</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s">'alphabet'</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">iterrows</span><span class="p">()</span>
<span class="p">])</span>
<span class="n">df_pyspark</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+--------+-----+
|alphabet|index|
+--------+-----+
|       p|    0|
|       y|    1|
|       s|    2|
|       p|    3|
|       a|    4|
|       r|    5|
|       k|    6|
+--------+-----+
</code></pre></div></div>


        </div>
        <label for="/2019/10/30/pandas_to_pyspark_before_0.19.2/" class="read-more-trigger"></label>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2019/10/14/notes-on-handling-rnn-training-problem/">
        Common issues in RNN training
      </a>
    </h1>

    <span class="post-date">14 Oct 2019</span>

    
        <p>Exploding gradients and vanishing gradients are two common issues with the training of RNN.</p>

<p>To avoid exploding gradients, one may use:</p>

<ul>
  <li>Truncated Back-propagation through time (BPTT)</li>
  <li>Clip gradients at threshold</li>
  <li>RMSprop to adjust learning rate</li>
</ul>

<p>Vanishing gradients are harder to detect. To avoid it, one may use:</p>

<ul>
  <li>Weight initialization</li>
  <li>ReLu activation functions</li>
  <li>RMSprop</li>
  <li>LSTM, GRUs</li>
</ul>

    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2019/09/25/pyspark-generate-sequences/">
        Generate sequence from an array column of pyspark dataframe
      </a>
    </h1>

    <span class="post-date">25 Sep 2019</span>

    
        <p>Suppose I have a Hive table that has a column of sequences,</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+------------------+
|          sequence|
+------------------+
|      [3, 23, 564]|
+------------------+
</code></pre></div></div>

<p>how to generate a column that contains permutations of each sequence in multiple rows? The desired output shall look like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+------------------+------------------+
|          sequence|       permutation|
+------------------+------------------+
|      [3, 23, 564]|      [3, 23, 564]|
|      [3, 23, 564]|      [3, 564, 23]|
|      [3, 23, 564]|      [23, 3, 564]|
|      [3, 23, 564]|      [23, 564, 3]|
|      [3, 23, 564]|      [564, 3, 23]|
|      [3, 23, 564]|      [564, 23, 3]|
+------------------+------------------+
</code></pre></div></div>

<p>In order to get multiple rows out of each row, we need to use the function <code class="highlighter-rouge">explode</code>. First, we write a user-defined function (UDF) to return the list of permutations given a array (sequence):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">Row</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">IntegerType</span><span class="p">,</span> <span class="n">ArrayType</span>

<span class="o">@</span><span class="n">udf_type</span><span class="p">(</span><span class="n">ArrayType</span><span class="p">(</span><span class="n">ArrayType</span><span class="p">(</span><span class="n">IntegerType</span><span class="p">())))</span>
<span class="k">def</span> <span class="nf">permutation</span><span class="p">(</span><span class="n">a_list</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="p">.</span><span class="n">permutations</span><span class="p">(</span><span class="n">a_list</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">a_list</span><span class="p">)))</span>
</code></pre></div></div>

<p>The <code class="highlighter-rouge">udf_type</code> function is adapted from the <a href="https://johnpaton.net/posts/clean-spark-udfs/">blog post by John Paton</a>. The output type is specified to be an array of ‚Äúarray of integers‚Äù.</p>

<p>The application of this function with <code class="highlighter-rouge">explode</code> will yield the result above:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">spark_session</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">sequence</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">564</span><span class="p">])])</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">'permutation'</span><span class="p">,</span> <span class="n">F</span><span class="p">.</span><span class="n">explode</span><span class="p">(</span><span class="n">permutation</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">col</span><span class="p">(</span><span class="s">'sequence'</span><span class="p">))))</span>
</code></pre></div></div>


    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2019/09/24/pyspark-could-not-serialize-object/">
        Pyspark error "Could not serialize object"
      </a>
    </h1>

    <span class="post-date">24 Sep 2019</span>

    
        <div>
            <p>This post is related to the idea discuss in the post <a href="https://www.placeiq.com/2017/11/how-to-solve-non-serializable-errors-when-instantiating-objects-in-spark-udfs/">‚ÄúHow to Solve Non-Serializable Errors When Instantiating Objects In Spark UDFs‚Äù</a>. Here I discuss the solution in a hypothetical scenario in pyspark.

        </div>
        <input type="checkbox" class="read-more-state" id="/2019/09/24/pyspark-could-not-serialize-object/"/>
        <div class="read-more">
            
Suppose I have a class to transform string to sum of numbers like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">udf</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">IntegerType</span>

<span class="k">class</span> <span class="nc">AnimalsToNumbers</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spark</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="s">'elephant'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'bear'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">'cat'</span><span class="p">:</span> <span class="mi">9</span><span class="p">}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_spark</span> <span class="o">=</span> <span class="n">spark</span>
        
    <span class="k">def</span> <span class="nf">addition</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">animal_str</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">_mapping</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">animal_str</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'+'</span><span class="p">)])</span>
    
    <span class="k">def</span> <span class="nf">add_up_animals</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="n">addition_udf</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">udf</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">addition</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">df</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">'sum'</span><span class="p">,</span> <span class="n">addition_udf</span><span class="p">(</span><span class="s">'animal'</span><span class="p">))</span>
</code></pre></div></div>

<p>(In practical cases, <code class="highlighter-rouge">self._mapping</code> is a huge object containing the dictionary and other attributes that are derived from methods in the <code class="highlighter-rouge">AnimalsToNumbers</code> class.) If I want to transform a dataframe <code class="highlighter-rouge">df</code> that looks like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+-------------+
|       animal|
+-------------+
|elephant+bear|
|     cat+bear|
+-------------+
</code></pre></div></div>

<p>The operation</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">AnimalsToNumbers</span><span class="p">(</span><span class="n">spark</span><span class="p">).</span><span class="n">add_up_animals</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div>

<p>will lead to an error like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------
Py4JError                                 Traceback (most recent call last)
~/anaconda3/envs/pyspark_demo/lib/python3.5/site-packages/pyspark/serializers.py in dumps(self, obj)
    589         try:
--&gt; 590             return cloudpickle.dumps(obj, 2)
    591         except pickle.PickleError:
...
PicklingError: Could not serialize object: Py4JError: An error occurred while calling o25.__getstate__. Trace:
py4j.Py4JException: Method __getstate__([]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)
	at py4j.Gateway.invoke(Gateway.java:274)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
</code></pre></div></div>

<p>The issue is that, as <code class="highlighter-rouge">self._mapping</code> appears in the function <code class="highlighter-rouge">addition</code>, when applying <code class="highlighter-rouge">addition_udf</code> to the pyspark dataframe, the object <code class="highlighter-rouge">self</code> (i.e. the <code class="highlighter-rouge">AnimalsToNumbers</code> class) has to be serialized but it can‚Äôt be.</p>

<p>A (surprisingly simple) way is to create a reference to the dictionary (<code class="highlighter-rouge">self._mapping</code>) but not the object:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AnimalsToNumbers</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spark</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="s">'elephant'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'bear'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">'cat'</span><span class="p">:</span> <span class="mi">9</span><span class="p">}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_spark</span> <span class="o">=</span> <span class="n">spark</span>
        
    
    <span class="k">def</span> <span class="nf">add_up_animals</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="n">mapping</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_mapping</span>
        <span class="k">def</span> <span class="nf">addition</span><span class="p">(</span><span class="n">animal_str</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">mapping</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">animal_str</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'+'</span><span class="p">)])</span>

        <span class="n">addition_udf</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">udf</span><span class="p">(</span><span class="n">addition</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">df</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">'sum'</span><span class="p">,</span> <span class="n">addition_udf</span><span class="p">(</span><span class="s">'animal'</span><span class="p">))</span>

</code></pre></div></div>

<p><code class="highlighter-rouge">AnimalsToNumbers(spark).add_up_animals(df).show()</code> would yield:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+-------------+---+
|       animal|sum|
+-------------+---+
|elephant+bear|  5|
|     cat+bear| 13|
+-------------+---+

</code></pre></div></div>

<p>Yay :)</p>


        </div>
        <label for="/2019/09/24/pyspark-could-not-serialize-object/" class="read-more-trigger"></label>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2019/09/16/json-in-postgresql/">
        Handling JSON in PostgreSQL
      </a>
    </h1>

    <span class="post-date">16 Sep 2019</span>

    
        <p>Found a useful <a href="https://devhints.io/postgresql-json">cheatsheet</a> that listed out operations on JSON in PostgreSQL.</p>

<p>If I want to list the rows where column <code class="highlighter-rouge">col1</code> in table <code class="highlighter-rouge">table1</code> contains a JSON object with the key <code class="highlighter-rouge">key1</code>, I can use:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">table1</span>
<span class="k">where</span> <span class="n">col1</span><span class="o">-&gt;</span><span class="s1">'key1'</span> <span class="k">IS</span> <span class="k">NOT</span> <span class="k">NULL</span>
</code></pre></div></div>

    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2019/09/15/lwa-package-updated-to-new-version/">
        Local wave activity package updated to version 0.3.7
      </a>
    </h1>

    <span class="post-date">15 Sep 2019</span>

    
        <p>The <a href="https://github.com/csyhuang/hn2016_falwa">python package</a> <code class="highlighter-rouge">hn2016_falwa</code> has just been updated with the following changes:</p>
<ul>
  <li>Switched the unittest framework from <code class="highlighter-rouge">unittest</code> to <code class="highlighter-rouge">pytest</code></li>
  <li>Improved interface of the QGField object</li>
  <li>Added a new function <code class="highlighter-rouge">hn2016_falwa.download_data.retrieve_erai</code> to download ERA-Interim data (but not connected to the main program yet)</li>
</ul>

<p>My package uses <code class="highlighter-rouge">f2py</code>. When switching from <code class="highlighter-rouge">unittest</code> to <code class="highlighter-rouge">pytest</code>, there are several changes to make in <code class="highlighter-rouge">setup.py</code> and <code class="highlighter-rouge">.travis</code> to accommodate such usage:</p>

<p>In <code class="highlighter-rouge">setup</code> (<code class="highlighter-rouge">numpy.distutils.core.setup</code>), remove the argument</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_suite</span><span class="o">=</span><span class="s">'tests.my_module_suite'</span>
</code></pre></div></div>
<p>and add:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">packages</span><span class="o">=</span><span class="n">find_packages</span><span class="p">(),</span>
    <span class="n">setup_requires</span><span class="o">=</span><span class="p">[</span><span class="s">'pytest-runner'</span><span class="p">],</span>
    <span class="n">tests_require</span><span class="o">=</span><span class="p">[</span><span class="s">'pytest'</span><span class="p">],</span>
</code></pre></div></div>

<p>In <code class="highlighter-rouge">.travis</code>, the <code class="highlighter-rouge">script</code> section looks like:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>script:
  - python setup.py pytest
</code></pre></div></div>

    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2019/09/10/colloquium-at-uw-madison/">
        Visit to AOS at UW-Madison
      </a>
    </h1>

    <span class="post-date">10 Sep 2019</span>

    
        <p>I visited the <a href="https://www.aos.wisc.edu/">Department of Atmospheric and Oceanic Sciences</a> at the <a href="https://www.wisc.edu/">University of Wisconsin-Madison</a> for two days and had a lot of fun discussing atmospheric (and machine learning) research with the scientists there. Thanks Prof. Jon Martin for inviting me over!</p>

<p>The colloquium I gave on Monday was an overview of the finite-amplitude local Rossby wave activity theory and its application to study blocking. We learned from this framework that atmospheric blocking can be modelled as a traffic jam problem. I also mentioned the follow-up work by <a href="https://journals.ametsoc.org/toc/atsc/current">Paradise et al. (2019, JAS)</a> that discusses the implication of this notion.</p>

<p>The slides for my colloquium can be found below:</p>

<p><a href="https://www.dropbox.com/s/ladgdo6tjkyhvmo/clare-uw-colloquium-20190909.pdf?dl=1"><img src="/assets/img/uw-colloquium-thumbnail.png" alt="Slides for UW-Madison Colloquium" /></a></p>

    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2019/07/24/rnn-papers/">
        Papers on architecture of Recurrent Neural Networks (RNN)
      </a>
    </h1>

    <span class="post-date">24 Jul 2019</span>

    
        <p>Bookmarking some papers mentioned in Andrew Ng‚Äôs course <a href="https://www.coursera.org/learn/nlp-sequence-models">Sequence Models</a>:</p>

<h3 id="gate-recurrent-unit-gru">Gate Recurrent Unit (GRU)</h3>

<ul>
  <li>
    <p>Cho, K., Van Merri√´nboer, B., Bahdanau, D., &amp; Bengio, Y. (2014). <a href="https://arxiv.org/abs/1409.1259">On the properties of neural machine translation: Encoder-decoder approaches</a>. <em>arXiv preprint arXiv:1409.1259</em>.</p>
  </li>
  <li>
    <p>Chung, J., Gulcehre, C., Cho, K., &amp; Bengio, Y. (2014). <a href="https://arxiv.org/abs/1412.3555">Empirical evaluation of gated recurrent neural networks on sequence modeling</a>. <em>arXiv preprint arXiv:1412.3555</em>.</p>
  </li>
</ul>

<h3 id="long-short-term-memory-lstm">Long short-term memory (LSTM)</h3>

<ul>
  <li>Hochreiter, S., &amp; Schmidhuber, J. (1997). <a href="https://www.bioinf.jku.at/publications/older/2604.pdf">Long short-term memory</a>. <em>Neural computation</em>, 9(8), 1735-1780.</li>
</ul>

<p>(More to update)</p>

    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2019/06/20/useful-git-commands/">
        Useful Git commands at work
      </a>
    </h1>

    <span class="post-date">20 Jun 2019</span>

    
        <div>
            <p>Below are solutions I curated online to solve problems related to Git when collaborating with others and working on several branches together. <em>This post will be updated from time to time.</em>

        </div>
        <input type="checkbox" class="read-more-state" id="/2019/06/20/useful-git-commands/"/>
        <div class="read-more">
            </p>
<h3 id="copy-a-file-from-one-branch-to-another">Copy a file from one branch to another</h3>
<p>To copy a file to the current branch from another branch (<a href="https://stackoverflow.com/questions/307579/how-do-i-copy-a-version-of-a-single-file-from-one-git-branch-to-another/7099164">ref</a>)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git checkout another_branch the_file_you_want.txt
</code></pre></div></div>

<h3 id="merge-changes-from-master-branch-to-yours">Merge changes from master branch to yours</h3>
<p>To merge changes from another branch to yours, you can use <code class="highlighter-rouge">merge</code> or <code class="highlighter-rouge">rebase</code> depending on the preferred commit order. BitBucket has a <a href="https://www.atlassian.com/git/tutorials/merging-vs-rebasing">nice tutorial</a> discussing the difference btween the two. Usually I‚Äôd love to have the changes pulled from another branch as a single commit with <code class="highlighter-rouge">git merge</code>.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git checkout master      # the branch with changes
git pull                 # pull the remote changes to local master branch
git checkout mybranch    # go back to mybranch
git merge master         # incorporate the changes into mybranch
</code></pre></div></div>

<h3 id="how-to-use-git-revert">How to use <code class="highlighter-rouge">git revert</code></h3>
<p><a href="https://www.atlassian.com/git/tutorials/undoing-changes/git-revert">Useful tutorial</a> from BitBucket on <code class="highlighter-rouge">git revert</code>.</p>

<h3 id="compare-differences-between-branches-and-output-the-results">Compare differences between branches and output the results</h3>
<p>If you want to compare the difference between your (more updated) branch and the master branch, use the command</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git diff master..your_branch
</code></pre></div></div>
<p>You can save the comparison results into a text file with colors by</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git diff master..your_branch &gt; your_branch_to_master.diff
</code></pre></div></div>
<p>The color can be viewed when you open the <code class="highlighter-rouge">.diff</code> file with <a href="https://www.sublimetext.com/">Sublime</a>.</p>

<h3 id="update-password-for-git-on-mac-os-x">Update password for Git on Mac OS X</h3>
<p>use the following command</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git config --global credential.helper osxkeychain
</code></pre></div></div>

<p>[to be continued]</p>


        </div>
        <label for="/2019/06/20/useful-git-commands/" class="read-more-trigger"></label>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2019/06/13/comparison-between-different-statistical-language-models/">
        Comparison between different statistical language models
      </a>
    </h1>

    <span class="post-date">13 Jun 2019</span>

    
        <p>When researching on different machine learning models for modeling languages (i.e. sequence model), here are some useful resources I found online:</p>

<h3 id="articles">Articles</h3>

<ul>
  <li><a href="https://www.alibabacloud.com/blog/hmm-memm-and-crf-a-comparative-analysis-of-statistical-modeling-methods_592049?spm=a2c41.11544604.0.0">HMM, MEMM, and CRF: A Comparative Analysis of Statistical Modeling Methods</a> by Alibaba Clouder</li>
  <li><a href="http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/">Introduction to Conditional Random Fields</a> by Edwin Chen</li>
  <li><a href="http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf">An Introduction to Conditional Random Fields</a> by Charles Sutton and Andrew McCallum</li>
  <li><a href="http://www.albertauyeung.com/post/python-sequence-labelling-with-crf/">Performing Sequence Labelling using CRF in Python</a> by Albert AuYeung</li>
  <li>(more to update)</li>
</ul>

<h3 id="courses">Courses</h3>
<ul>
  <li><a href="http://www.yisongyue.com/courses/cs155/2016_winter/">Machine Learning &amp; Data Mining</a> by <a href="mailto:yyue@caltech.edu">Yisong Yue</a> at CalTech</li>
</ul>

<!-- https://towardsdatascience.com/conditional-random-field-tutorial-in-pytorch-ca0d04499463

http://web.cse.ohio-state.edu/~fosler-lussier.1/papers/IEEE_CRF_SALP_FoslerEtalPreprint.pdf

https://web.stanford.edu/~jurafsky/asru09.pdf

https://towardsdatascience.com/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c

http://www.robots.ox.ac.uk/~szheng/papers/CRFasRNN.pdf


https://blog.paperspace.com/recurrent-neural-networks-part-1-2/

https://medium.com/deep-writing

https://www.quora.com/Are-recurrent-neural-networks-RNNs-considered-a-generative-model-in-Machine-Learning


https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213

 -->

    
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>

<div>
<p><!-- hitwebcounter Code START -->
<a href="http://www.hitwebcounter.com" target="_blank">
<img src="http://hitwebcounter.com/counter/counter.php?page=6563193&style=0006&nbdigits=5&type=page&initCount=0" title="my widget for counting" Alt="my widget for counting (since Dec24, 2016)"   border="0" >
</a> <br/>
</p>
</div>
  <link rel="stylesheet" href="/public/css/trial.css">

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
