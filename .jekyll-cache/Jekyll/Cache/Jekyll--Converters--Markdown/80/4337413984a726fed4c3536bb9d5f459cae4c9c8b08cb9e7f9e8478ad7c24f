I"Ó<p>These are notes taken from the Coursera course about <a href="https://www.coursera.org/learn/competitive-data-science">competitive data science</a>.</p>

<h2 id="types-of-features">Types of features</h2>
<ul>
  <li>Ordinal (with an order)</li>
  <li>Categorical</li>
  <li>ID (unique for each entity)</li>
  <li>Numeric</li>
  <li>Text</li>
  <li>Datetime and coordinates</li>
</ul>

<h2 id="numerical-variables">Numerical variables</h2>
<h3 id="preprocessing-non-tree-based-model">Preprocessing: Non tree-based model</h3>
<p>Linear models, SVM, kNN and NN have their performance dependent on scaling of features. 
In general, when training a non-tree-based model, using preprocessing to scale all the 
features to one scale make their initial impact roughly similar.
Different types of scaling includes:</p>
<ul>
  <li>Scale to [0,1], e.g. sklearn.preprocessing.MinMaxScaler</li>
  <li>Scale to mean = 0, std = 1, e.g. sklearn.preprocessing.StandardScaler</li>
  <li>Log-transform: np.log(1+x)</li>
  <li>Raising to the power &lt; 1: np.sqrt(x+2/3)</li>
  <li>Rank transformation: setting spaces between proper assorted values to be equal.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rank</span><span class="p">([</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">1e+5</span><span class="p">])</span> <span class="o">==</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
<span class="n">rank</span><span class="p">([</span><span class="mi">1000</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span> <span class="o">==</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>    </div>
    <p>or</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">rankdata</span>
</code></pre></div>    </div>
  </li>
  <li>Treatment of outliers: clip features values between two chosen values of lower bound 
and upper bound, e.g. 1st and 99th percentiles. This procedure of clipping is well-known 
in financial data and it is called <a href="https://en.wikipedia.org/wiki/Winsorizing">winsorization</a>. 
In python, it can be done as follows:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">upperbound</span><span class="p">,</span> <span class="n">lowerbound</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">x</span><span class="p">,[</span><span class="mi">1</span><span class="p">,</span><span class="mi">99</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">upperbound</span><span class="p">,</span> <span class="n">lowerbound</span><span class="p">)</span>
</code></pre></div>    </div>
    <h3 id="preprocessing-tree-based-model">Preprocessing: Tree-based model</h3>
  </li>
  <li>Independent of scaling</li>
</ul>

<h3 id="feature-generation">Feature generation</h3>
<p>Generate features easier to handle by models using prior knowledge/insights from EDA. In general,
regardless of whether a model is tree-based, handling multiplication/division is difficult.</p>
<ul>
  <li>Housing price: given area and price, compute price per square-meter</li>
  <li>Knowing vertical and horizontal distance from water source, derive direct distance</li>
  <li>Price of products: using <em>fractional part</em> (e.g. 0.99 in 2.99) as a feature 
(psychological effect!)</li>
  <li>Price in an auction: to distinguish whether it is a human (15M) or a robot (9.35245233M)</li>
</ul>

<p><a href="https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/">Useful article on feature engineering</a></p>

<p>(To be continued, stopped at week 1 - numerical features)</p>
:ET